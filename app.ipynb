{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dee8b4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from pptx import Presentation\n",
    "from typing import TypedDict\n",
    "import pdb\n",
    "import sys\n",
    "import re\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21d9fb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added url\n",
      "added url\n",
      "added url\n"
     ]
    }
   ],
   "source": [
    "\n",
    "REGEX = re.compile(\"https:\\\\/\\\\/www.linkedin.com\\\\/in\\\\/[^\\\\/]+\")\n",
    "\n",
    "# get the urls to scrape\n",
    "def get_urls() -> list[str]:\n",
    "    urls = []\n",
    "    url = \" \"\n",
    "    while True:\n",
    "        url = input(\"Enter url (leave empty for end): \")\n",
    "        if REGEX.match(url):\n",
    "            print(\"added url\")\n",
    "            urls.append(url)\n",
    "        elif len(url) > 0:\n",
    "            print(\"Url is not valid\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return urls\n",
    "\n",
    "\n",
    "urls = get_urls()\n",
    "\n",
    "# found no urls\n",
    "if len(urls) == 0:\n",
    "    print(\"no urls found\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "def setup_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    service = Service()\n",
    "    return webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "\n",
    "driver = setup_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a565501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(url):\n",
    "    driver.get(url)\n",
    "    print(f\"visiting {url}\")\n",
    "\n",
    "\n",
    "def signin():\n",
    "    input(\"Press enter when signed in: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc082c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience(TypedDict):\n",
    "    company: str\n",
    "    roles: list[str]\n",
    "    dates: list[str]\n",
    "\n",
    "\n",
    "def scrape_single_exp(element: WebElement):\n",
    "    title_link_ele = element.find_elements(By.TAG_NAME, \"a\")[1]\n",
    "    lines = title_link_ele.text.splitlines()[::2]\n",
    "\n",
    "    experience: Experience = {\n",
    "        \"company\": lines[1].split(\"路\")[0],\n",
    "        \"roles\": [lines[0]],\n",
    "        \"dates\": [lines[2].split(\"路\")[0]],\n",
    "    }\n",
    "\n",
    "    return experience\n",
    "\n",
    "\n",
    "def scrape_multi_exp(title_element: WebElement, exp_elements: WebElement):\n",
    "    title_link_ele = title_element.find_elements(By.TAG_NAME, \"a\")[1]\n",
    "    # print(title_link_ele.text)\n",
    "    # print(title_link_ele)\n",
    "    experience: Experience = {\n",
    "        \"company\": title_link_ele.text.splitlines()[0].split(\"路\")[0],\n",
    "        \"roles\": [],\n",
    "        \"dates\": [],\n",
    "    }\n",
    "\n",
    "    for exp_element in exp_elements:\n",
    "        link_element = exp_element.find_element(By.TAG_NAME, \"a\")\n",
    "        lines = link_element.text.splitlines()[::2]\n",
    "        experience[\"roles\"].append(lines[0])\n",
    "        experience[\"dates\"].append(lines[1].split(\"路\")[0])\n",
    "\n",
    "    return experience\n",
    "\n",
    "\n",
    "def scrape_experience(element: WebElement) -> Experience:\n",
    "    # find experience start\n",
    "    # print(element.text[::2])\n",
    "\n",
    "    edu_div = element.find_element(\n",
    "        By.XPATH, \".//div[@data-view-name='profile-component-entity']\"\n",
    "    )\n",
    "\n",
    "    # print(edu_div.text)\n",
    "\n",
    "    # find multi experience start if it exists\n",
    "    multi_exps = edu_div.find_elements(\n",
    "        By.XPATH, \".//div[@data-view-name='profile-component-entity']\"\n",
    "    )\n",
    "    if len(multi_exps) > 0:\n",
    "        print(f\"found {len(multi_exps)} sub experiences\")\n",
    "        return scrape_multi_exp(edu_div, multi_exps)\n",
    "    else:\n",
    "        print(f\"found a sub experience\")\n",
    "        return scrape_single_exp(edu_div)\n",
    "\n",
    "\n",
    "def scrape_experiences() -> list[Experience]:\n",
    "    # find education section\n",
    "    section = driver.find_element(By.XPATH, \"//section[.//div[@id='experience']]\")\n",
    "\n",
    "    # find education items\n",
    "    exp_list = section.find_element(By.TAG_NAME, \"ul\")\n",
    "    exp_items = exp_list.find_elements(By.XPATH, \"./li\")\n",
    "\n",
    "    print(f\"found {len(exp_items)} main experiences\")\n",
    "    experiences: list[Experience] = []\n",
    "    for exp in exp_items:\n",
    "        experiences.append(scrape_experience(exp))\n",
    "\n",
    "    return experiences\n",
    "\n",
    "def print_experiences(experiences:list[Experience]):\n",
    "    print(\"\\nEXPERIENCES:\")\n",
    "    for exp in experiences:\n",
    "        print(exp['company'])\n",
    "        for sub_exp in zip(exp[\"roles\"],exp[\"dates\"]):\n",
    "            print(sub_exp[0])\n",
    "            print(sub_exp[1])\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34666e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visiting https://www.linkedin.com/in/patrick-m-obrien/\n",
      "found 4 main experiences\n",
      "found a sub experience\n",
      "found a sub experience\n",
      "found a sub experience\n",
      "found 2 sub experiences\n",
      "\n",
      "EXPERIENCES:\n",
      "Greenwich Consulting Group LLC \n",
      "Founder & CEO\n",
      "Nov 2022 - Present \n",
      "\n",
      "Egon Zehnder\n",
      "Partner and Global Insurance Practice Leader\n",
      "Sep 2010 - Nov 2022 \n",
      "\n",
      "Allstate\n",
      "President, Allstate Roadside Services\n",
      "Jun 2008 - Sep 2010 \n",
      "\n",
      "GE\n",
      "President, Partnership Marketing Group\n",
      "Jan 2007 - Jun 2008 \n",
      "President, Accident & Health Division, GE Insurance\n",
      "Mar 2005 - Jan 2007 \n",
      "\n",
      "visiting https://www.linkedin.com/in/nate-westfall/\n",
      "waiting ten seconds for page load\n",
      "found 5 main experiences\n",
      "found a sub experience\n",
      "found a sub experience\n",
      "found a sub experience\n",
      "found a sub experience\n",
      "found a sub experience\n",
      "\n",
      "EXPERIENCES:\n",
      "Full Plate Project, YMCA\n",
      "Volunteer\n",
      "Jun 2021 - Present \n",
      "\n",
      "Worcester Polytechnic Institute \n",
      "Teaching Assistant\n",
      "Jun 2025 - Aug 2025 \n",
      "\n",
      "Gotta Have It Inc \n",
      "Web Designer\n",
      "Jun 2024 - Aug 2024 \n",
      "\n",
      "Gotta Have It Inc\n",
      "Web Designer\n",
      "Jun 2023 - Aug 2023 \n",
      "\n",
      "YMCA Southcoast\n",
      "Intern\n",
      "Jun 2022 - Aug 2022 \n",
      "\n",
      "visiting https://www.linkedin.com/in/jdunphy/\n",
      "waiting ten seconds for page load\n",
      "found 5 main experiences\n",
      "found a sub experience\n",
      "found a sub experience\n",
      "found a sub experience\n",
      "found a sub experience\n",
      "found a sub experience\n",
      "\n",
      "EXPERIENCES:\n",
      "Waymo\n",
      "Hardware Engineer\n",
      "Nov 2017 - Present \n",
      "\n",
      "Google\n",
      "Optics Hardware Engineering Lead\n",
      "Jun 2013 - Nov 2017 \n",
      "\n",
      "Synaptics\n",
      "Sr. Staff System Architect\n",
      "Sep 2011 - Jun 2013 \n",
      "\n",
      "Solyndra\n",
      "Director of Test Development\n",
      "Jul 2007 - Sep 2011 \n",
      "\n",
      "Texas Instruments\n",
      "Systems Engineer\n",
      "Jul 2006 - Jul 2007 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_url(urls[0])\n",
    "signin()\n",
    "\n",
    "\n",
    "experiences = scrape_experiences()\n",
    "print_experiences(experiences)\n",
    "\n",
    "for index in range(1, len(urls)):\n",
    "    get_url(urls[index])\n",
    "    print(\"waiting ten seconds for page load\")\n",
    "    time.sleep(10)\n",
    "    experiences = scrape_experiences()\n",
    "    print_experiences(experiences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4c0f013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = Presentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd26d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.get(\"https://www.linkedin.com/in/jdunphy/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "409242fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "hours=3\n",
    "next_hours=3.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
